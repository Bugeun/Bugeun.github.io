import os
import feedparser
import requests
from bs4 import BeautifulSoup
import html2text
from datetime import datetime

# Blogger RSS í”¼ë“œ URL
BLOG_FEED_URL = "https://bugeun.blogspot.com/feeds/posts/default?alt=rss"

# í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ê¸°ë³¸ URL
T_BLOG_URL = "https://bugeun1007.tistory.com/category/Posts"
PAGE_PARAM = "?page="

# ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥ ê²½ë¡œ
POSTS_DIR = r"C:\Users\choib\OneDrive\ë¬¸ì„œ\GitHub\bugeun.github.io\_posts\Blogger"
T_POSTS_DIR = r"C:\Users\choib\OneDrive\ë¬¸ì„œ\GitHub\bugeun.github.io\_posts\Tistory"
os.makedirs(POSTS_DIR, exist_ok=True)
os.makedirs(T_POSTS_DIR, exist_ok=True)

# User-Agent ì„¤ì • (í¬ë¡¤ë§ ì°¨ë‹¨ ë°©ì§€)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
}

# HTML -> Markdown ë³€í™˜ê¸°
converter = html2text.HTML2Text()
converter.ignore_links = False  # ë§í¬ ìœ ì§€

# âœ… 1. Blogger RSS íŒŒì‹±
print("\nğŸŒ Blogger RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œì‘!")

# RSS ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
feed = feedparser.parse(BLOG_FEED_URL)

# ë¸”ë¡œê·¸ ê¸€ íŒŒì‹± ë° Markdown ì €ì¥
for entry in feed.entries:
    title = entry.title
    date_string = entry.published  # ì˜ˆ: "Wed, 05 Feb 2025 04:14:00 +0000"
    date_object = datetime.strptime(date_string, "%a, %d %b %Y %H:%M:%S %z")
    date = date_object.date()
    
    content_html = entry.content[0].value if "content" in entry else entry.summary
    content_md = converter.handle(content_html)

    # íŒŒì¼ ì´ë¦„ ì •ë¦¬
    safe_title = title.replace(" ", "-").replace("/", "-").replace("?", "").replace(":", "").lower()
    filename = f"{date}-{safe_title}.md"
    filepath = os.path.join(POSTS_DIR, filename)

    # Markdown íŒŒì¼ ì €ì¥
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(f"---\n")
        f.write(f"title: \"{title}\"\n")
        f.write(f"date: {date}\n")
        f.write(f"categories:\n")
        f.write(f"- Vulnerability Reports\n")
        f.write(f"---\n\n")
        f.write(f'<a href = \"{entry.link}\">link here</a>\n')
        f.write(content_md)


    print(f"âœ… {filename} ì €ì¥ ì™„ë£Œ!")

print("\nğŸ‰ ëª¨ë“  Blogger ê¸€ì„ Markdownìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ!")

# âœ… 2. Tistory í¬ë¡¤ë§ (ëª¨ë“  í˜ì´ì§€)
print(f"\nğŸŒ í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘!")

page = 1  # ì²« ë²ˆì§¸ í˜ì´ì§€ë¶€í„° ì‹œì‘
while True:
    url = T_BLOG_URL + PAGE_PARAM + str(page)
    print(f"ğŸ“„ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘: {url}")

    # í˜ì´ì§€ ìš”ì²­
    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ (HTTP {response.status_code})")
        break  # ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìŒ

    # HTML íŒŒì‹±
    soup = BeautifulSoup(response.text, "html.parser")

    # í˜„ì¬ í˜ì´ì§€ì— ê¸€ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if not soup.find("div", class_="list_content") and not soup.find("div", class_="entryProtected"):
        print(f"âœ… í˜ì´ì§€ {page}ì— ê²Œì‹œê¸€ì´ ì—†ìŒ. í¬ë¡¤ë§ ì¢…ë£Œ!")
        break

    # âœ… ë³´í˜¸ëœ ê¸€ íŒŒì‹± (entryProtected)
    for entry in soup.find_all("div", class_="entryProtected"):
        title_tag = entry.find("a")
        date_tag = entry.find("p", class_="date")

        if title_tag and date_tag:
            title = title_tag.text.strip()
            post_url = "https://bugeun1007.tistory.com" + title_tag["href"]

            # ë‚ ì§œ ë³€í™˜
            date_text = date_tag.text.strip().split(" ë³´í˜¸ê¸€")[0]
            date_object = datetime.strptime(date_text, "%Y. %m. %d. %H:%M")
            post_date = date_object.date()

            # ê°œë³„ ê¸€ HTML ê°€ì ¸ì˜¤ê¸°
            post_response = requests.get(post_url, headers=HEADERS)
            post_soup = BeautifulSoup(post_response.text, "html.parser")

            # ë³¸ë¬¸ ì¶”ì¶œ
            content_div = post_soup.find("div", class_="tt_article_useless_p_margin")
            content_html = str(content_div) if content_div else "<p>ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.</p>"

            # HTML -> Markdown ë³€í™˜
            content_md = converter.handle(post_url)

            # íŒŒì¼ ì €ì¥
            safe_title = title.replace(" ", "-").replace("/", "-").replace("?", "").replace(":", "").lower()
            filename = f"{post_date}-{safe_title}.md"
            filepath = os.path.join(T_POSTS_DIR, filename)

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(f"---\n")
                f.write(f"title: \"{title}\"\n")
                f.write(f"date: {post_date}\n")
                f.write(f"categories: Articles,Papers\n")
                f.write(f"---\n\n")
                f.write(content_md)

            print(f"âœ… {filename} ì €ì¥ ì™„ë£Œ!")

    # âœ… ì¼ë°˜ ê¸€ íŒŒì‹± (list_content)
    for entry in soup.find_all("div", class_="list_content"):
        title_tag = entry.find("a", class_="item")
        info_content = entry.find("div", class_="info_content")

        if title_tag and info_content:
            title = info_content.find("strong").text.strip()
            post_url = "https://bugeun1007.tistory.com" + title_tag["href"]

            # ë‚ ì§œ ì¶”ì¶œ ë° ë³€í™˜
            raw_text = info_content.text.strip()
            date_text = raw_text.split("|")[-1].strip()
            date_object = datetime.strptime(date_text, "%Y. %m. %d. %H:%M")
            post_date = date_object.date()

            # ê°œë³„ ê¸€ HTML ê°€ì ¸ì˜¤ê¸°
            post_response = requests.get(post_url, headers=HEADERS)
            post_soup = BeautifulSoup(post_response.text, "html.parser")

            # ë³¸ë¬¸ ì¶”ì¶œ
            content_div = post_soup.find("div", class_="tt_article_useless_p_margin")
            content_html = str(content_div) if content_div else "<p>ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.</p>"

            # HTML -> Markdown ë³€í™˜
            content_md = converter.handle(content_html)

            # íŒŒì¼ ì €ì¥
            safe_title = title.replace(" ", "-").replace("/", "-").replace("?", "").replace(":", "").lower()
            filename = f"{post_date}-{safe_title}.md"
            filepath = os.path.join(T_POSTS_DIR, filename)

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(f"---\n")
                f.write(f"title: \"{title}\"\n")
                f.write(f"date: {post_date}\n")
                f.write(f"categories: Blog\n")
                f.write(f"---\n\n")
                f.write(content_md)

            print(f"âœ… {filename} ì €ì¥ ì™„ë£Œ!")

    # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
    page += 1


    # âœ… ì¼ë°˜ ì½”ë”© ê¸€ íŒŒì‹± (list_content)

while True:
    T_BLOG_URL = "https://bugeun1007.tistory.com/category/Coding"
    url = T_BLOG_URL + PAGE_PARAM + str(page)
    print(f"ğŸ“„ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘: {url}")

    # í˜ì´ì§€ ìš”ì²­
    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ (HTTP {response.status_code})")
        break  # ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìŒ
    

    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥ ê²½ë¡œ
    T_POSTS_DIR = r"C:\Users\choib\OneDrive\ë¬¸ì„œ\GitHub\bugeun.github.io\_posts\Coding"
    os.makedirs(T_POSTS_DIR, exist_ok=True)

    # User-Agent ì„¤ì • (í¬ë¡¤ë§ ì°¨ë‹¨ ë°©ì§€)
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
    }

    # HTML -> Markdown ë³€í™˜ê¸°
    converter = html2text.HTML2Text()
    converter.ignore_links = False  # ë§í¬ ìœ ì§€


    # HTML íŒŒì‹±
    soup = BeautifulSoup(response.text, "html.parser")

    # í˜„ì¬ í˜ì´ì§€ì— ê¸€ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if not soup.find("div", class_="list_content") and not soup.find("div", class_="entryProtected"):
        print(f"âœ… í˜ì´ì§€ {page}ì— ê²Œì‹œê¸€ì´ ì—†ìŒ. í¬ë¡¤ë§ ì¢…ë£Œ!")
        break

    for entry in soup.find_all("div", class_="list_content"):
        title_tag = entry.find("a", class_="item")
        info_content = entry.find("div", class_="info_content")

        if title_tag and info_content:
            title = info_content.find("strong").text.strip()
            post_url = "https://bugeun1007.tistory.com" + title_tag["href"]

            # ë‚ ì§œ ì¶”ì¶œ ë° ë³€í™˜
            raw_text = info_content.text.strip()
            date_text = raw_text.split("|")[-1].strip()
            date_object = datetime.strptime(date_text, "%Y. %m. %d. %H:%M")
            post_date = date_object.date()

            # ê°œë³„ ê¸€ HTML ê°€ì ¸ì˜¤ê¸°
            post_response = requests.get(post_url, headers=HEADERS)
            post_soup = BeautifulSoup(post_response.text, "html.parser")

            # ë³¸ë¬¸ ì¶”ì¶œ
            content_div = post_soup.find("div", class_="tt_article_useless_p_margin")
            content_html = str(content_div) if content_div else "<p>ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.</p>"

            # HTML -> Markdown ë³€í™˜
            content_md = converter.handle(content_html)

            # íŒŒì¼ ì €ì¥
            safe_title = title.replace(" ", "-").replace("/", "-").replace("?", "").replace(":", "").lower()
            filename = f"{post_date}-{safe_title}.md"
            filepath = os.path.join(T_POSTS_DIR, filename)

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(f"---\n")
                f.write(f"title: \"{title}\"\n")
                f.write(f"date: {post_date}\n")
                f.write(f"categories: Coding\n")
                f.write(f"---\n\n")
                f.write(content_md)

            print(f"âœ… {filename} ì €ì¥ ì™„ë£Œ!")

    # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
    page += 1

print("\nğŸ‰ ëª¨ë“  Blogger & Tistory ê¸€ì„ Markdownìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ!")
